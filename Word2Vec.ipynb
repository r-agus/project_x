{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5754beb",
   "metadata": {},
   "source": [
    "## Word2Vec: Document Vector Representation\n",
    "Representation of documents as the average of the embeddings of the\n",
    "words they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4797d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords as sw\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt_tab' , quiet=True)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05ce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "traindata = pd.read_csv('Datasets/EvaluationData/politicES_phase_2_train_public.csv', header=0)\n",
    "ytrain = traindata.iloc[:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59749693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spanish stopwords from nltk\n",
    "spanish_sw = set(sw.words('spanish'))\n",
    "extras = {'rt', 'https', 'http', 'jaja', 'jajaja', 'jajajaja', 'jajajajaja', 'mas', 'hace'}\n",
    "stopwords = spanish_sw.union(extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b5d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spanish \n",
    "stemmer = SnowballStemmer('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b124727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_letters(text: str, letters: list) -> str:\n",
    "    placeholders = {letter: f\"__PLACEHOLDER_{i}__\" for i, letter in enumerate(letters)}\n",
    "    for k, v in placeholders.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join(ch for ch in text if not unicodedata.combining(ch))\n",
    "    for k, v in placeholders.items():\n",
    "        text = text.replace(v, k)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bf817cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_stem(text, letters=['ñ','Ñ']):\n",
    "    text = preserve_letters(text, letters)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-ZñÑáéíóúüÁÉÍÓÚÜ\\s]\", \"\", text)    \n",
    "    tokens = text.split()\n",
    "    tokens_stemmed = [stemmer.stem(w) for w in tokens]\n",
    "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords]\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5a57173",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ytrain.iloc[:, -1].astype(str).apply(preprocess_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ec023b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_w2v = Word2Vec(\n",
    "        sentences=tokens.tolist(),\n",
    "        vector_size=100,      \n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=1\n",
    "    )\n",
    "except Exception as e:\n",
    "    # print(f\"Error training Word2Vec model: {e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc438726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_vector(tokens):\n",
    "    words_vecs = [\n",
    "        model_w2v.wv[word] for word in tokens if word in model_w2v.wv\n",
    "    ]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(model_w2v.vector_size)\n",
    "    return np.mean(words_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "86e0eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('porqu', 0.7838612198829651), ('obstant', 0.7752715945243835), ('dimitiri', 0.7621756792068481), ('despartidiz', 0.7608611583709717), ('deseari', 0.7568363547325134), ('ocurrir', 0.7545263767242432), ('digal', 0.7542862296104431), ('adivinais', 0.7540134191513062), ('explicari', 0.7535245418548584), ('abrumador', 0.7519144415855408)]\n"
     ]
    }
   ],
   "source": [
    "word = \"si\"\n",
    "try:\n",
    "    print(model_w2v.wv.most_similar(word))\n",
    "except KeyError:\n",
    "    print(f\"The word {word} is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10533151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the words is not in the vocabulary: \"Key 'gobierno' not present\"\n"
     ]
    }
   ],
   "source": [
    "word1 = \"gobierno\"\n",
    "word2 = \"croquetas\"\n",
    "\n",
    "try:\n",
    "    sim = model_w2v.wv.similarity(word1, word2)\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {sim:.4f}\")\n",
    "except KeyError as e:\n",
    "    print(f\"One of the words is not in the vocabulary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c00cb8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29435\n",
      "['user', 'par', 'tod', 'com', 'si', 'politici', 'hashtag', 'per', 'hac', 'politicalparty', 'hoy', 'gobiern', 'ser', 'tien', 'sobr', 'much', 'nuestr', 'españ', 'pued', 'sol']\n"
     ]
    }
   ],
   "source": [
    "print(len(model_w2v.wv.key_to_index))\n",
    "print(list(model_w2v.wv.key_to_index.keys())[:20])  # primeras 20 palabras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330719db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
